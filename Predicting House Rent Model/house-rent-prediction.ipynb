{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4097760,"sourceType":"datasetVersion","datasetId":2355600}],"dockerImageVersionId":30615,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-13T03:46:26.196883Z","iopub.execute_input":"2023-12-13T03:46:26.197458Z","iopub.status.idle":"2023-12-13T03:46:26.574481Z","shell.execute_reply.started":"2023-12-13T03:46:26.197423Z","shell.execute_reply":"2023-12-13T03:46:26.573406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(color_codes=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:26.579551Z","iopub.execute_input":"2023-12-13T03:46:26.580076Z","iopub.status.idle":"2023-12-13T03:46:27.140882Z","shell.execute_reply.started":"2023-12-13T03:46:26.580046Z","shell.execute_reply":"2023-12-13T03:46:27.139710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/house-rent-prediction-dataset/House_Rent_Dataset.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:27.142385Z","iopub.execute_input":"2023-12-13T03:46:27.142735Z","iopub.status.idle":"2023-12-13T03:46:27.181125Z","shell.execute_reply.started":"2023-12-13T03:46:27.142703Z","shell.execute_reply":"2023-12-13T03:46:27.180100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Remove unwanted \"Posted On\" column\ndf.drop(columns = \"Posted On\",inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:27.183730Z","iopub.execute_input":"2023-12-13T03:46:27.184073Z","iopub.status.idle":"2023-12-13T03:46:27.199927Z","shell.execute_reply.started":"2023-12-13T03:46:27.184041Z","shell.execute_reply":"2023-12-13T03:46:27.198933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking unique values in all object-datatypes column\ndf.select_dtypes(include='object').nunique()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:27.201330Z","iopub.execute_input":"2023-12-13T03:46:27.202149Z","iopub.status.idle":"2023-12-13T03:46:27.219067Z","shell.execute_reply.started":"2023-12-13T03:46:27.202114Z","shell.execute_reply":"2023-12-13T03:46:27.218304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove column Floor and Area Locality column due to amount of unique values\ndf.drop(columns=['Floor','Area Locality'], inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:27.220288Z","iopub.execute_input":"2023-12-13T03:46:27.220797Z","iopub.status.idle":"2023-12-13T03:46:27.237425Z","shell.execute_reply.started":"2023-12-13T03:46:27.220757Z","shell.execute_reply":"2023-12-13T03:46:27.236281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"# List all category to plot\ncategory = ['Area Type', 'City','Furnishing Status','Tenant Preferred','Point of Contact']\n\n# create subplots figures\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20,10))\naxs = axs.ravel()\n\n# create barplot for each category\nfor i, var in enumerate(category):\n    sns.barplot(x=var,y='Rent' ,data=df ,ax=axs[i], estimator=np.mean)\n    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=90)\n\n#autofit layout\nfig.tight_layout()\n\n#remove 6th subplot\nfig.delaxes(axs[5])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:27.239072Z","iopub.execute_input":"2023-12-13T03:46:27.239772Z","iopub.status.idle":"2023-12-13T03:46:29.285110Z","shell.execute_reply.started":"2023-12-13T03:46:27.239737Z","shell.execute_reply":"2023-12-13T03:46:29.283966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify no of max category\nmax_categories = 5\ncategory = ['Area Type', 'City', 'Furnishing Status', 'Tenant Preferred', 'Point of Contact']\n\n#create fig and axs\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15,15))\n\n# create piechart\nfor i, var in enumerate(category):\n    if 1 < len(axs.flat):\n        # count no of occurence in each category\n        cat_counts = df[var].value_counts()\n        \n        # group category as other that are not the top max categories\n        if len(cat_counts) > max_categories:\n            top_cat_counts = cat_counts[:max_categories]\n            others_cat_counts = pd.Series(cat_counts[max_categories:].sum(),index=['Other'])\n            cat_counts = pd.concat([top_cat_counts, others_cat_counts])\n            \n        # create a pie chart\n        axs.flat[i].pie(cat_counts, labels=cat_counts.index, autopct='%1.1f%%', startangle=90)\n        \n        # set title\n        axs.flat[i].set_title(f'{var} Distribution')\n\n# remove 6th subplot\nfig.delaxes(axs[1,2])\n\n# autofit layout\nfig.tight_layout()\n\nplt.show()        ","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:29.286557Z","iopub.execute_input":"2023-12-13T03:46:29.286915Z","iopub.status.idle":"2023-12-13T03:46:30.257481Z","shell.execute_reply.started":"2023-12-13T03:46:29.286882Z","shell.execute_reply":"2023-12-13T03:46:30.256155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_vars = ['Size', 'BHK', 'Bathroom']\n\nfig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20,10))\naxs = axs.flatten()\n\nfor i, var in enumerate (num_vars):\n    sns.boxplot(x=var, data=df, ax=axs[i])\n    \nfig.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:30.258993Z","iopub.execute_input":"2023-12-13T03:46:30.259435Z","iopub.status.idle":"2023-12-13T03:46:31.182139Z","shell.execute_reply.started":"2023-12-13T03:46:30.259396Z","shell.execute_reply":"2023-12-13T03:46:31.180894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_vars = ['Size', 'BHK', 'Bathroom']\n\nfig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20,10))\naxs = axs.flatten()\n\nfor i, var in enumerate (num_vars):\n    sns.violinplot(x=var, data=df, ax=axs[i])\n    \nfig.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:31.183687Z","iopub.execute_input":"2023-12-13T03:46:31.184106Z","iopub.status.idle":"2023-12-13T03:46:32.183014Z","shell.execute_reply.started":"2023-12-13T03:46:31.184066Z","shell.execute_reply":"2023-12-13T03:46:32.181953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# check missing value\ncheck_miss = df.isnull().sum() * 100 / df.shape[0]\ncheck_miss[check_miss > 0].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:32.184428Z","iopub.execute_input":"2023-12-13T03:46:32.185021Z","iopub.status.idle":"2023-12-13T03:46:32.195281Z","shell.execute_reply.started":"2023-12-13T03:46:32.184990Z","shell.execute_reply":"2023-12-13T03:46:32.194164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:32.196771Z","iopub.execute_input":"2023-12-13T03:46:32.197073Z","iopub.status.idle":"2023-12-13T03:46:32.208302Z","shell.execute_reply.started":"2023-12-13T03:46:32.197046Z","shell.execute_reply":"2023-12-13T03:46:32.207163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label Encoding for Each Datatype","metadata":{}},{"cell_type":"code","source":"# Loop over each column in DF where dtype is \"object\"\nfor col in df.select_dtypes(include=['object']).columns:\n    # print unique column and its values\n    print(f\"{col}: {df[col].unique()}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:32.212529Z","iopub.execute_input":"2023-12-13T03:46:32.212900Z","iopub.status.idle":"2023-12-13T03:46:32.222601Z","shell.execute_reply.started":"2023-12-13T03:46:32.212869Z","shell.execute_reply":"2023-12-13T03:46:32.221561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\n# Loop over each column in DF where dtype is \"object\"\nfor col in df.select_dtypes(include=['object']).columns:\n    \n    # initialize label encoder\n    label_encoder = preprocessing.LabelEncoder()\n    \n    # fit encoder to the unique values in column\n    label_encoder.fit(df[col].unique())\n    \n    # transform column using encoder\n    df[col] = label_encoder.transform(df[col])\n    \n    # print unique column and its encoded values\n    print(f\"{col}: {df[col].unique()}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:32.224037Z","iopub.execute_input":"2023-12-13T03:46:32.224418Z","iopub.status.idle":"2023-12-13T03:46:32.320128Z","shell.execute_reply.started":"2023-12-13T03:46:32.224388Z","shell.execute_reply":"2023-12-13T03:46:32.319392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Heatmap Correlation\nplt.figure(figsize=(20,16))\nsns.heatmap(df.corr(), fmt=\".2g\", annot=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:32.321564Z","iopub.execute_input":"2023-12-13T03:46:32.321880Z","iopub.status.idle":"2023-12-13T03:46:33.197431Z","shell.execute_reply.started":"2023-12-13T03:46:32.321851Z","shell.execute_reply":"2023-12-13T03:46:33.196133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#select features (x) and variables (y)\nX = df.drop('Rent', axis=1)\ny = df['Rent']\n\n# split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:33.198961Z","iopub.execute_input":"2023-12-13T03:46:33.199416Z","iopub.status.idle":"2023-12-13T03:46:33.423501Z","shell.execute_reply.started":"2023-12-13T03:46:33.199375Z","shell.execute_reply":"2023-12-13T03:46:33.422496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove Outlier from train data using Z-score","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\n#Define columns to remove outliers\nselect_cols = ['Size', 'BHK', 'Bathroom']\n\n# calculate z-score for select_cols in training data\nz_scores = np.abs(stats.zscore(X_train[select_cols]))\n\n# set threshold for outlier detection, use 3\nthreshold = 3\n\n# find the indices of outliers based on threshold\noutlier_indices = np.where(z_scores > threshold)[0]\n\n# remove the outliers from training_data\nX_train = X_train.drop(X_train.index[outlier_indices])\ny_train = y_train.drop(y_train.index[outlier_indices])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:33.424920Z","iopub.execute_input":"2023-12-13T03:46:33.425520Z","iopub.status.idle":"2023-12-13T03:46:33.437974Z","shell.execute_reply.started":"2023-12-13T03:46:33.425485Z","shell.execute_reply":"2023-12-13T03:46:33.436924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Regressor","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import fetch_california_housing\n\n# Create DecisionTreeRegressor object\ndtree = DecisionTreeRegressor()\n\n# Define the hyperparameters to tune and their values\nparam_grid = {\n    'max_depth': [2,4,6,8],\n    'min_samples_split': [2,4,6,8],\n    'min_samples_leaf': [1,2,3,4],\n    'max_features': ['auto','sqrt','log2'],\n    'random_state': [0,42]\n}\n\n# Create a GridSearchCV object\ngrid_search = GridSearchCV(dtree, param_grid, cv=5, scoring='neg_mean_squared_error')\n\n# Fit the GridSearchCV object to the data\ngrid_search.fit(X_train, y_train)\n\n# Print the best hyperparameters\nprint(grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:33.439544Z","iopub.execute_input":"2023-12-13T03:46:33.440420Z","iopub.status.idle":"2023-12-13T03:46:46.165398Z","shell.execute_reply.started":"2023-12-13T03:46:33.440387Z","shell.execute_reply":"2023-12-13T03:46:46.164445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtree = DecisionTreeRegressor(random_state=0, max_depth=8, max_features='log2', min_samples_leaf=4, min_samples_split=2)\ndtree.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:46.169306Z","iopub.execute_input":"2023-12-13T03:46:46.169637Z","iopub.status.idle":"2023-12-13T03:46:46.183836Z","shell.execute_reply.started":"2023-12-13T03:46:46.169609Z","shell.execute_reply":"2023-12-13T03:46:46.182980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import mean_absolute_percentage_error\nimport math\n\ny_pred = dtree.predict(X_test)\n\n# Mean Absolute Error (MAE)\nmae = metrics.mean_absolute_error(y_test, y_pred)\n\n# Mean Absolute Percentage Error (MAPE)\nmape = mean_absolute_percentage_error(y_test, y_pred) * 100\n\n# Mean Squared Error (MSE)\nmse = metrics.mean_squared_error(y_test, y_pred)\n\n# R^2 score\nr2 = metrics.r2_score(y_test, y_pred)\n\n# Root Mean Squared Error (RMSE)\nrmse = math.sqrt(mse)\n\n# Print the calculated metrics\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"MAPE: {mape:.2f}%\")\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"R^2: {r2:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:46.185204Z","iopub.execute_input":"2023-12-13T03:46:46.185824Z","iopub.status.idle":"2023-12-13T03:46:46.196146Z","shell.execute_reply.started":"2023-12-13T03:46:46.185793Z","shell.execute_reply":"2023-12-13T03:46:46.195343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lap_df = pd.DataFrame({\n  \"Feature Name\": X_train.columns,\n  \"Importance\": dtree.feature_importances_\n})\n\n# Sort features by importance in descending order\nf1 = lap_df.sort_values(by='Importance', ascending=False)\nprint(f1)\n\nf12 = f1.head(10)\nplt.figure(figsize=(10,8))\n\nsns.barplot(x=\"Importance\", y=\"Feature Name\", data=f12)\nplt.xlabel(\"Importance\", fontsize =16)\nplt.ylabel(\"Feature Name\", fontsize =16)\nplt.title(\"Feature Importance in Decision Tree\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:46.197541Z","iopub.execute_input":"2023-12-13T03:46:46.198135Z","iopub.status.idle":"2023-12-13T03:46:46.573716Z","shell.execute_reply.started":"2023-12-13T03:46:46.198105Z","shell.execute_reply":"2023-12-13T03:46:46.572658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install shap\n\nfrom shap import TreeExplainer\nimport shap\n\nexplainer = TreeExplainer(dtree)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:46:46.576047Z","iopub.execute_input":"2023-12-13T03:46:46.576792Z","iopub.status.idle":"2023-12-13T03:47:04.214841Z","shell.execute_reply.started":"2023-12-13T03:46:46.576760Z","shell.execute_reply":"2023-12-13T03:47:04.213550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.Explainer(dtree, X_test)\nshap_values = explainer(X_test)\nshap.plots.waterfall(shap_values[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:47:04.216518Z","iopub.execute_input":"2023-12-13T03:47:04.217196Z","iopub.status.idle":"2023-12-13T03:47:05.147730Z","shell.execute_reply.started":"2023-12-13T03:47:04.217150Z","shell.execute_reply":"2023-12-13T03:47:05.146581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# create randomforestregressor object\nrf = RandomForestRegressor()\n\n# define hyperparameters\nparam_grid = {\n    'max_depth': [3,5,7,9],\n    'min_samples_split': [2,5,10],\n    'min_samples_leaf': [1,2,4],\n    'max_features': ['auto','sqrt'],\n    'random_state': [0,42]\n}\n\n# Create a GridSearchCV object\ngrid_search = GridSearchCV(rf, param_grid, cv=5, scoring='r2')\n\n# Fit the GridSearchCV object to the data\ngrid_search.fit(X_train, y_train)\n\n# Print the best hyperparameters\nprint(grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:47:05.149096Z","iopub.execute_input":"2023-12-13T03:47:05.149730Z","iopub.status.idle":"2023-12-13T03:50:13.854448Z","shell.execute_reply.started":"2023-12-13T03:47:05.149689Z","shell.execute_reply":"2023-12-13T03:50:13.853370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=42, max_depth=9, min_samples_split=5, min_samples_leaf=2, max_features='sqrt')\n\n#Fit it\nrf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:50:13.855971Z","iopub.execute_input":"2023-12-13T03:50:13.856560Z","iopub.status.idle":"2023-12-13T03:50:14.104558Z","shell.execute_reply.started":"2023-12-13T03:50:13.856526Z","shell.execute_reply":"2023-12-13T03:50:14.103490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import mean_absolute_percentage_error\nimport math\n\ny_pred = rf.predict(X_test)\n\n# Mean Absolute Error (MAE)\nmae = metrics.mean_absolute_error(y_test, y_pred)\n\n# Mean Absolute Percentage Error (MAPE)\nmape = mean_absolute_percentage_error(y_test, y_pred) * 100\n\n# Mean Squared Error (MSE)\nmse = metrics.mean_squared_error(y_test, y_pred)\n\n# R^2 score\nr2 = metrics.r2_score(y_test, y_pred)\n\n# Root Mean Squared Error (RMSE)\nrmse = math.sqrt(mse)\n\n# Print the calculated metrics\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"MAPE: {mape:.2f}%\")\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"R^2: {r2:.4f}\")\nprint(f\"RMSE: {rmse:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:50:14.105738Z","iopub.execute_input":"2023-12-13T03:50:14.106024Z","iopub.status.idle":"2023-12-13T03:50:14.133955Z","shell.execute_reply.started":"2023-12-13T03:50:14.105999Z","shell.execute_reply":"2023-12-13T03:50:14.132833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imp_df = pd.DataFrame({\n  \"Feature Name\": X_train.columns,\n  \"Importance\": rf.feature_importances_\n})\n\n# Sort features by importance in descending order\nf1 = lap_df.sort_values(by='Importance', ascending=False)\nprint(f1)\n\nf12 = f1.head(10)\nplt.figure(figsize=(10,8))\n\nsns.barplot(x=\"Importance\", y=\"Feature Name\", data=f12)\nplt.xlabel(\"Importance\", fontsize =16)\nplt.ylabel(\"Feature Name\", fontsize =16)\nplt.title(\"Feature Importance in Random Forest Regressor\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:50:14.135370Z","iopub.execute_input":"2023-12-13T03:50:14.135843Z","iopub.status.idle":"2023-12-13T03:50:14.489225Z","shell.execute_reply.started":"2023-12-13T03:50:14.135798Z","shell.execute_reply":"2023-12-13T03:50:14.487950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from shap import TreeExplainer\n\nexplainer = TreeExplainer(rf)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:50:14.490999Z","iopub.execute_input":"2023-12-13T03:50:14.491444Z","iopub.status.idle":"2023-12-13T03:50:19.920946Z","shell.execute_reply.started":"2023-12-13T03:50:14.491403Z","shell.execute_reply":"2023-12-13T03:50:19.920161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.Explainer(rf, X_test)\nshap_values = explainer(X_test)\nshap.plots.waterfall(shap_values[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:50:19.922068Z","iopub.execute_input":"2023-12-13T03:50:19.922912Z","iopub.status.idle":"2023-12-13T03:50:38.011063Z","shell.execute_reply.started":"2023-12-13T03:50:19.922878Z","shell.execute_reply":"2023-12-13T03:50:38.009876Z"},"trusted":true},"execution_count":null,"outputs":[]}]}